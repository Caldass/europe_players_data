{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "#Code to insert into df the scraping information as dicts\n",
    "#Code found on https://stackoverflow.com/questions/41825868/update-python-dictionary-add-another-value-to-existing-key/41826126#41826126\n",
    "#Wrote by user https://stackoverflow.com/users/1860929/mu-%e7%84%a1\n",
    "def set_key(dictionary, key, value):\n",
    "     if key not in dictionary:\n",
    "         dictionary[key] = value\n",
    "     elif type(dictionary[key]) == list:\n",
    "         dictionary[key].append(value)\n",
    "     else:\n",
    "         dictionary[key] = [dictionary[key], value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_url = 'https://www.transfermarkt.us'\n",
    "europe_url = 'https://www.transfermarkt.us/wettbewerbe/europa'\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36'}\n",
    "seconds = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run this only if you haven't scraped any data yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Final Dataframe with player informations\n",
    "df = pd.DataFrame()\n",
    "\n",
    "#Creates soup and request object\n",
    "response_league = requests.get(europe_url, time.sleep(seconds), headers = headers)\n",
    "\n",
    "soup_league = BeautifulSoup(response_league.content, 'html.parser', from_encoding=\"iso-8859-1\")\n",
    "\n",
    "#Lists that we'll store data we still need to scrape and data we already scraped\n",
    "league_links = []\n",
    "league_links_done = []\n",
    "\n",
    "team_links = []\n",
    "team_links_done = []\n",
    "\n",
    "\n",
    "player_links = []\n",
    "player_links_done = []\n",
    "\n",
    "#Gets league link\n",
    "for league in soup_league.find('tbody').findAll(href = True, title = True):\n",
    "    if league['href'].split('/')[2] == 'startseite':\n",
    "        league_links.append(league['href'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start running this after alrealdy scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do not run the cell above if it's the first time you're scraping the data, only the next one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading df\n",
    "df = pd.read_csv('scrapped_df.csv')\n",
    "\n",
    "#Loading lists\n",
    "with open(\"lists_file.txt\", 'rb') as f:\n",
    "    league_links, league_links_done, team_links, team_links_done, player_links, player_links_done = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start running this if you haven't scraped any data yet\n",
    "\n",
    "#create copy of list so that it allows us to use remove items from the list as we iterate through it\n",
    "league_links[:] = [league_tup for league_tup in league_links if league_tup not in league_links_done]\n",
    "\n",
    "#Opens link for each league\n",
    "for league_url in league_links[:]:\n",
    "    #Creates league url\n",
    "    league_profile_url = main_url + league_url\n",
    "    \n",
    "    #Gets response for url\n",
    "    response_league = requests.get(league_profile_url , time.sleep(seconds), headers = headers)\n",
    "    \n",
    "    #Creates bs object\n",
    "    soup_league = BeautifulSoup(response_league.content, 'html.parser', from_encoding=\"iso-8859-1\")\n",
    "    \n",
    "    #Gets each team's url for the league and appends to the team_links list if it's the first time scraping    \n",
    "    for team in soup_league.findAll('a',  href = True, attrs = {\"class\": \"vereinprofil_tooltip\"}):\n",
    "        if (team['href'] not in team_links) & (team['href'].split('/')[2] == 'startseite'):\n",
    "            team_links.append(team['href'])\n",
    "    \n",
    "    #Gets league country\n",
    "    league_country = soup_league.find('div', attrs = {\"class\" : \"flagge\"}).find('img').get('title')\n",
    "    \n",
    "    #since there could be players from sub-20 teams, the league name get's confused, this will avoid that from happening\n",
    "    league_name = soup_league.findAll('h1', attrs = {\"class\" : \"spielername-profil\"})[0].text\n",
    "    \n",
    "    #create copy of list so that it allows us to use remove items from the list as we iterate through it\n",
    "    team_links[:] = [team_tup for team_tup in team_links if team_tup not in team_links_done]\n",
    "    \n",
    "    for club in team_links[:]:\n",
    "        #Defines the last season year depending if the season is per year or per semester\n",
    "        if '/' in soup_league.find('h2', attrs = {\"class\" : \"table-header\"}).text.split(' ')[-1]:\n",
    "            this_season = int('20' + soup_league.find('h2', attrs = {\"class\" : \"table-header\"}).text.split(' ')[-1].split('/')[0])\n",
    "        else:\n",
    "            this_season = int(soup_league.find('h2', attrs = {\"class\" : \"table-header\"}).text.split(' ')[-1]) - 1\n",
    "        \n",
    "        #Gets team url for each team in the teams list\n",
    "        team_profile_url = main_url + club\n",
    "        \n",
    "        #Gets response for url\n",
    "        response_team = requests.get(team_profile_url , time.sleep(seconds), headers = headers)\n",
    "\n",
    "        #Creates bs object\n",
    "        soup_team = BeautifulSoup(response_team.content, 'html.parser', from_encoding=\"iso-8859-1\")\n",
    "        \n",
    "        #some clubs do not have data but still show up in the list, this will prevent the code from stopping\n",
    "        try:\n",
    "            soup_team.find(attrs = {\"class\" : \"empty\"}).text\n",
    "        except:\n",
    "            #Get profile for each player of the team and appends it to player_links        \n",
    "            for player_url in soup_team.find('table', attrs = {\"class\": \"items\"}).findAll('a',  href = True, attrs = {\"class\": \"spielprofil_tooltip\"}):\n",
    "                if player_url['href'] not in player_links:\n",
    "                    player_links.append(player_url['href'])\n",
    "\n",
    "\n",
    "            \n",
    "            #create copy of list so that it allows us to use remove items from the list as we iterate through it\n",
    "            player_links[:] = [player_tup for player_tup in player_links if player_tup not in player_links_done]\n",
    "\n",
    "            #Gets information for each player of the club\n",
    "            for player in player_links[:]:\n",
    "\n",
    "                #Adds players profile link to the url page\n",
    "                player_profile_url =  main_url + player\n",
    "\n",
    "                #Creates link to player's season stats\n",
    "                player_stats_url = player_profile_url.replace('profil', 'leistungsdaten') + \"/plus/0?saison=\" + str(this_season)\n",
    "                player_last_stats_url = player_profile_url.replace('profil', 'leistungsdaten') + \"/plus/0?saison=\" + str(this_season - 1)\n",
    "\n",
    "\n",
    "                #Gets response for prifle url\n",
    "                response_player = requests.get(player_profile_url , time.sleep(seconds), headers = headers)\n",
    "\n",
    "                #Gets response for stats url\n",
    "                response_player_stats = requests.get(player_stats_url , time.sleep(seconds), headers = headers)\n",
    "                response_player_last_stats = requests.get(player_last_stats_url , time.sleep(seconds), headers = headers)\n",
    "\n",
    "\n",
    "                #Creates bs object\n",
    "                soup_player = BeautifulSoup(response_player.content, 'html.parser', from_encoding=\"iso-8859-1\")\n",
    "\n",
    "                #Creates bs object\n",
    "                soup_player_stats = BeautifulSoup(response_player_stats.content, 'html.parser', from_encoding=\"iso-8859-1\")\n",
    "                soup_player_last_stats = BeautifulSoup(response_player_last_stats.content, 'html.parser', from_encoding=\"iso-8859-1\")\n",
    "\n",
    "\n",
    "                 #Creates dict that we'll store the first step of the scraping\n",
    "                info = {}\n",
    "\n",
    "                #Gets most information from the player's profile\n",
    "                for z in soup_player.find('table', attrs = { \"class\" : \"auflistung\"}).findAll('tr'):\n",
    "                    set_key(info, z.th.text.strip().replace(':',''),  z.td.text.strip())\n",
    "\n",
    "                #Scraping extra information and stats\n",
    "                if info['Position'] != 'Goalkeeper':\n",
    "\n",
    "                    #Season scrapped\n",
    "                    try:\n",
    "                        set_key(info, 'current_season', soup_player_stats.findAll('td', attrs = {\"class\" : \"rechts\"})[0].text.split(' ')[-1].split(':')[0])\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    #Checks if player has season stats to scrap\n",
    "                    try :\n",
    "                        soup_player_stats.find(attrs = {\"class\" : \"empty\"}).text                    \n",
    "                    except:\n",
    "\n",
    "                        #Appearances\n",
    "                        try:\n",
    "                            set_key(info, 'appearances', soup_player_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[0].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Goals\n",
    "                        try:\n",
    "                            set_key(info, 'goals', soup_player_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[1].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Assists\n",
    "                        try:\n",
    "                            set_key(info, 'assists', soup_player_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[2].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Yellow cards\n",
    "                        try:\n",
    "                            set_key(info, 'yellow', soup_player_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[3].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Second Yellow cards\n",
    "                        try:\n",
    "                            set_key(info, 'second_yellow', soup_player_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[4].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Red\n",
    "                        try:\n",
    "                            set_key(info, 'red', soup_player_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[5].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Minutes played\n",
    "                        try:\n",
    "                            set_key(info, 'minutes_played', soup_player_stats.findAll('td', attrs = {\"class\" : \"rechts\"})[1].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Last season stats -------------------------------------------------------------------------\n",
    "                       #Checks if player has last season stats to scrap\n",
    "                    try :\n",
    "                        soup_player_last_stats.find(attrs = {\"class\" : \"empty\"}).text                    \n",
    "                    except:\n",
    "                            #Appearances\n",
    "                        try:\n",
    "                            set_key(info, 'last_appearances', soup_player_last_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[0].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Goals\n",
    "                        try:\n",
    "                            set_key(info, 'last_goals', soup_player_last_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[1].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Assists\n",
    "                        try:\n",
    "                            set_key(info, 'last_assists', soup_player_last_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[2].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Yellow cards\n",
    "                        try:\n",
    "                            set_key(info, 'last_yellow', soup_player_last_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[3].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Second Yellow cards\n",
    "                        try:\n",
    "                            set_key(info, 'last_second_yellow', soup_player_last_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[4].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Red\n",
    "                        try:\n",
    "                            set_key(info, 'last_red', soup_player_last_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[5].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Minutes played\n",
    "                        try:\n",
    "                            set_key(info, 'last_minutes_played', soup_player_last_stats.findAll('td', attrs = {\"class\" : \"rechts\"})[1].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    #Alternative name\n",
    "                    try:\n",
    "                        set_key(info, 'short_name', soup_player.find('h1', attrs = { \"itemprop\" : \"name\"}).text)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    #League\n",
    "                    try:\n",
    "                        set_key(info, 'league', league_name)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    #League country\n",
    "                    try:\n",
    "                        set_key(info, 'league_country', league_country)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    #Market value\n",
    "                    try:\n",
    "                        set_key(info, 'latest_market_value', soup_player.findAll('div', attrs = { \"class\" : \"right-td\"})[0].text.strip().split(' ')[0])\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    #Highest ever market value\n",
    "                    try:\n",
    "                        set_key(info, 'highest_market_value', soup_player.findAll('div', attrs = { \"class\" : \"right-td\"})[2].text.strip().split(' ')[0])\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    #Main position, three different excepts because depending on the player, the html can be in a different side\n",
    "                    try:\n",
    "                        set_key(info, 'main_position', soup_player.find('div', attrs = { \"class\" : \"hauptposition-left\"}).text.strip().split(':')[1])\n",
    "                    except:\n",
    "                        try:\n",
    "                            set_key(info, 'main_position', soup_player.find('div', attrs = { \"class\" : \"hauptposition-right\"}).text.strip().split(':')[1])\n",
    "                        except:\n",
    "                            try:\n",
    "                                set_key(info, 'main_position', soup_player.find('div', attrs = { \"class\" : \"hauptposition-center\"}).text.strip().split(':')[1])\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                    #Secondary position\n",
    "                    try:\n",
    "                        set_key(info, 'secondary_position', soup_player.find('div', attrs = { \"class\" : \"nebenpositionen\"}).text.strip().split('\\n')[1])\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "\n",
    "                #In case the player is a keeper, extra scraping needs to be different\n",
    "                else:                \n",
    "                    #Season scrapped\n",
    "                    try:\n",
    "                        set_key(info, 'current_season', soup_player_stats.findAll('td', attrs = {\"class\" : \"rechts\"})[0].text.split(' ')[-1].split(':')[0])\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    #Checks if keeper has season stats to scrap\n",
    "                    try:\n",
    "                        soup_player_stats.find(attrs = {\"class\" : \"empty\"}).text\n",
    "                    except:\n",
    "\n",
    "                        #Appearances\n",
    "                        try:\n",
    "                            set_key(info, 'appearances', soup_player_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[0].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Goals\n",
    "                        try:\n",
    "                            set_key(info, 'goals', soup_player_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[1].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Yellow cards\n",
    "                        try:\n",
    "                            set_key(info, 'yellow', soup_player_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[2].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Second Yellow cards\n",
    "                        try:\n",
    "                            set_key(info, 'second_yellow', soup_player_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[3].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Red cards\n",
    "                        try:\n",
    "                            set_key(info, 'red', soup_player_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[4].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Goals conceeded\n",
    "                        try:\n",
    "                            set_key(info, 'goals_conceded', soup_player_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[5].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Clean sheets\n",
    "                        try:\n",
    "                            set_key(info, 'clean_sheets', soup_player_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[6].text)\n",
    "                        except:\n",
    "                            pass\n",
    "                        #minutes played\n",
    "                        try:\n",
    "                            set_key(info, 'minutes_played', soup_player_stats.findAll('td', attrs = {\"class\" : \"rechts\"})[1].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Last season stats --------------------------\n",
    "                        #Appearances\n",
    "                        #Checks if keeper has last season stats to scrap\n",
    "                    try :\n",
    "                        soup_player_last_stats.find(attrs = {\"class\" : \"empty\"}).text                    \n",
    "                    except:\n",
    "                        try:\n",
    "                            set_key(info, 'last_appearances', soup_player_last_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[0].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Goals\n",
    "                        try:\n",
    "                            set_key(info, 'last_goals', soup_player_last_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[1].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Yellow cards\n",
    "                        try:\n",
    "                            set_key(info, 'last_yellow', soup_player_last_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[2].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Second Yellow cards\n",
    "                        try:\n",
    "                            set_key(info, 'last_second_yellow', soup_player_last_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[3].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Red cards\n",
    "                        try:\n",
    "                            set_key(info, 'last_red', soup_player_last_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[4].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Goals conceeded\n",
    "                        try:\n",
    "                            set_key(info, 'last_goals_conceded', soup_player_last_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[5].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Clean sheets\n",
    "                        try:\n",
    "                            set_key(info, 'last_clean_sheets', soup_player_last_stats.findAll('td', attrs = {\"class\" : \"zentriert\"})[6].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        #Minutes played\n",
    "                        try:\n",
    "                            set_key(info, 'last_minutes_played', soup_player_last_stats.findAll('td', attrs = {\"class\" : \"rechts\"})[1].text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                    #Alternative name\n",
    "                    try:\n",
    "                        set_key(info, 'short_name', soup_player.find('h1', attrs = { \"itemprop\" : \"name\"}).text)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                     #League\n",
    "                    try:\n",
    "                        set_key(info, 'league', league_name)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                      #League\n",
    "                    try:\n",
    "                        set_key(info, 'league_country', league_country)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    #Market value\n",
    "                    try:\n",
    "                        set_key(info, 'latest_market_value', soup_player.findAll('div', attrs = { \"class\" : \"right-td\"})[0].text.strip().split(' ')[0])\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                    #Highest ever market value\n",
    "                    try:\n",
    "                        set_key(info, 'highest_market_value', soup_player.findAll('div', attrs = { \"class\" : \"right-td\"})[2].text.strip().split(' ')[0])\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                info = pd.DataFrame.from_dict(info, orient = 'index').transpose()\n",
    "                df = pd.concat([df,info])\n",
    "\n",
    "                #Stores players already scraped\n",
    "                player_links_done.append(player)                \n",
    "\n",
    "        #Stores teams already scraped\n",
    "        team_links_done.append(club)\n",
    "        \n",
    "    #Store leagues already scraped\n",
    "    league_links_done.append(league_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run if the df is not done yet\n",
    "df.to_csv('scraped_df.csv', index = False)\n",
    "\n",
    "#Saving variables\n",
    "with open(\"lists_file.txt\", 'wb') as f:\n",
    "    pickle.dump((league_links, league_links_done, team_links, team_links_done, player_links, player_links_done), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
